#+TITLE: Submitting job scripts on LUMI
#+AUTHOR: Anthony Scemama
#+LANGUAGE:  en
#+HTML_LINK_UP: index.html
#+HTML_LINK_HOME: 02-Intro_qp.html
#+SETUPFILE: org-html-themes/org/theme-readtheorg-local.setup

  #+begin_important
To run Quantum Package (QP) and CHAMP codes properly, some environment
variables should be set. To set up the environment, you should source
the =/project/project_465000321/environment.sh= file in your shell
when you log in:

  #+begin_src bash
source /project/project_465000321/environment.sh
  #+end_src
  #+end_important


  This file contains the following commands:

#+begin_src bash
module load cray-python
source /project/project_465000321/qp2/quantum_package.rc   # source QP environment
#+end_src

When working with SLURM, you should use =sbatch= to submit your QP and CHAMP job
scripts. For example, to submit a QP job script named =job_qp.sh=, you would use
the command:

#+begin_src bash
sbatch job_qp.sh
#+end_src


* Example of a QP job script on LUMI

An example of a QP job script would be:

#+begin_src bash :tangle job_qp.sh
#!/bin/bash
#SBATCH --account=project_465000321
#SBATCH --time=00:20:00
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=128
#SBATCH --mem=0
#SBATCH --partition=standard

source /project/project_465000321/environment.sh

qp_srun fci COH2.ezfio > COH2.fci.out

#+end_src

  Note that QP uses shared memory parallelism with OpenMP. It will be
  efficient using all possible threads on the node.

* Example of a CHAMP job script

  For CHAMP, you should request one or many nodes with a certain number of
  processes (as there are a certain number of cores per node). There
  each MPI process uses a single thread.

An example of a CHAMP Variational Monte Carlo job script would be:

 #+begin_src bash
#SBATCH --job-name=champ        # Job name
#SBATCH --output=champ.o%j      # Name of stdout output file
#SBATCH --error=champ.e%j       # Name of stderr error file
#SBATCH --partition=standard    # Partition (queue) name
#SBATCH --nodes=1               # Total number of nodes
#SBATCH --ntasks=128            # Total number of mpi tasks
#SBATCH --mem=0                 # Allocate all the memory on the node
#SBATCH --time=0:20:00          # Run time (d-hh:mm:ss)
#SBATCH --mail-type=all         # Send email at begin and end of job
#SBATCH --account=project_465000321  # Project for billing

source /project/project_465000321/environment.sh

export PMI_NO_PREINITIALIZE=y

INPUT=vmc_h2o_hf.inp

OUTPUT=${INPUT%.inp}.out
srun /project/project_465000321/champ/bin/vmc.mov1 -i $INPUT -o $OUTPUT -e error
 #+end_src

An example of a CHAMP Diffusion Monte Carlo job script would be (along with the VMC calculation):

 #+begin_src bash
#SBATCH --job-name=champ        # Job name
#SBATCH --output=champ.o%j      # Name of stdout output file
#SBATCH --error=champ.e%j       # Name of stderr error file
#SBATCH --partition=standard    # Partition (queue) name
#SBATCH --nodes=1               # Total number of nodes
#SBATCH --ntasks=128            # Total number of mpi tasks
#SBATCH --mem=0                 # Allocate all the memory on the node
#SBATCH --time=0:20:00          # Run time (d-hh:mm:ss)
#SBATCH --mail-type=all         # Send email at begin and end of job
#SBATCH --account=project_465000321  # Project for billing


source /project/project_465000321/environment.sh

export PMI_NO_PREINITIALIZE=y

VMCINPUT=vmc_h2o.inp
VMCOUTPUT=${VMCINPUT%.inp}.out

DMCINPUT=dmc_h2o.inp
DMCOUTPUT=${DMCINPUT%.inp}.out

# Launch MPI code

srun /project/project_465000321/champ/bin/vmc.mov1  -i $VMCINPUT -o $VMCOUTPUT -e error

cat mc_configs_new* >> mc_configs
rm mc_configs_new*

srun /project/project_465000321/champ/bin/dmc.mov1 -i $DMCINPUT -o $DMCOUTPUT -e error

rm problem*
rm mc_configs_new*

 #+end_src

